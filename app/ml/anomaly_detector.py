"""
ì´ìƒ í–‰ë™ íƒì§€ ì‹œìŠ¤í…œ
ì•„ì´ì˜ í‰ìƒì‹œ êµ¬ë§¤ íŒ¨í„´ê³¼ ë‹¤ë¥¸ ì´ìƒ í–‰ë™ì„ ê°ì§€í•˜ê³  ì•Œë¦¼ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any, Optional
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.covariance import EllipticEnvelope
import joblib
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

class AnomalyDetector:
    def __init__(self):
        self.isolation_forest = IsolationForest(
            contamination=0.1, 
            random_state=42,
            n_estimators=100
        )
        self.elliptic_envelope = EllipticEnvelope(
            contamination=0.1,
            random_state=42
        )
        self.scaler = StandardScaler()
        self.model_path = Path("app/ml/models")
        self.model_path.mkdir(parents=True, exist_ok=True)
        
        # ì´ìƒ í–‰ë™ ìœ í˜• ì •ì˜
        self.anomaly_types = {
            'spending_spike': {
                'title': 'ê¸‰ê²©í•œ ì§€ì¶œ ì¦ê°€',
                'description': 'í‰ì†Œë³´ë‹¤ ì§€ì¶œì´ í¬ê²Œ ì¦ê°€í–ˆìŠµë‹ˆë‹¤',
                'severity': 'warning',
                'icon': 'ğŸ’°'
            },
            'category_shift': {
                'title': 'êµ¬ë§¤ íŒ¨í„´ ë³€í™”',
                'description': 'í‰ì†Œì™€ ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ì˜ êµ¬ë§¤ê°€ ì¦ê°€í–ˆìŠµë‹ˆë‹¤',
                'severity': 'info',
                'icon': 'ğŸ”„'
            },
            'frequency_change': {
                'title': 'êµ¬ë§¤ ë¹ˆë„ ë³€í™”',
                'description': 'êµ¬ë§¤ ë¹ˆë„ê°€ ê¸‰ê²©íˆ ë³€í™”í–ˆìŠµë‹ˆë‹¤',
                'severity': 'warning',
                'icon': 'ğŸ“Š'
            },
            'time_pattern_shift': {
                'title': 'ì‹œê°„ íŒ¨í„´ ë³€í™”',
                'description': 'í‰ì†Œì™€ ë‹¤ë¥¸ ì‹œê°„ëŒ€ì— êµ¬ë§¤ê°€ ì§‘ì¤‘ë˜ì—ˆìŠµë‹ˆë‹¤',
                'severity': 'info',
                'icon': 'â°'
            },
            'impulse_buying': {
                'title': 'ì¶©ë™ êµ¬ë§¤ ì˜ì‹¬',
                'description': 'ì§§ì€ ì‹œê°„ì— ë§ì€ êµ¬ë§¤ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤',
                'severity': 'warning',
                'icon': 'âš¡'
            },
            'emotional_shopping': {
                'title': 'ê°ì •ì  ì†Œë¹„ íŒ¨í„´',
                'description': 'ìŠ¤íŠ¸ë ˆìŠ¤ë‚˜ ê°ì • ë³€í™”ë¡œ ì¸í•œ ì†Œë¹„ íŒ¨í„´ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤',
                'severity': 'alert',
                'icon': 'ğŸ˜”'
            }
        }
    
    def extract_behavioral_features(self, df: pd.DataFrame, window_days: int = 7) -> Dict[str, float]:
        """í–‰ë™ ë¶„ì„ì„ ìœ„í•œ íŠ¹ì„± ì¶”ì¶œ"""
        if df.empty:
            return self._get_default_behavioral_features()
        
        # ê¸°ê°„ë³„ ë°ì´í„° ë¶„í• 
        now = datetime.now()
        recent_period = now - timedelta(days=window_days)
        
        recent_data = df[pd.to_datetime(df['timestamp']) >= recent_period]
        historical_data = df[pd.to_datetime(df['timestamp']) < recent_period]
        
        if recent_data.empty:
            return self._get_default_behavioral_features()
        
        # ìµœê·¼ ê¸°ê°„ íŠ¹ì„±
        recent_features = self._calculate_period_features(recent_data)
        
        # ê¸°ì¤€ íŠ¹ì„± (ê³¼ê±° ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
        if not historical_data.empty:
            historical_features = self._calculate_period_features(historical_data)
            # ë³€í™”ëŸ‰ ê³„ì‚°
            change_features = self._calculate_change_features(recent_features, historical_features)
        else:
            change_features = {}
        
        # ì „ì²´ íŠ¹ì„± í†µí•©
        features = {
            **recent_features,
            **change_features,
            'data_availability': len(historical_data) / len(df) if len(df) > 0 else 0
        }
        
        return features
    
    def _calculate_period_features(self, df: pd.DataFrame) -> Dict[str, float]:
        """íŠ¹ì • ê¸°ê°„ì˜ êµ¬ë§¤ íŠ¹ì„± ê³„ì‚°"""
        if df.empty:
            return self._get_default_period_features()
        
        # ê¸°ë³¸ í†µê³„
        df['total_amount'] = df['price'] * df['cnt']
        total_spent = df['total_amount'].sum()
        total_purchases = len(df)
        avg_amount = total_spent / total_purchases if total_purchases > 0 else 0
        
        # ì¹´í…Œê³ ë¦¬ ë¶„í¬
        category_dist = df['type'].value_counts(normalize=True).to_dict()
        
        # ì‹œê°„ íŒ¨í„´
        df['hour'] = pd.to_datetime(df['timestamp']).dt.hour
        df['weekday'] = pd.to_datetime(df['timestamp']).dt.weekday
        
        time_variance = df['hour'].var() if len(df) > 1 else 0
        weekday_variance = df['weekday'].var() if len(df) > 1 else 0
        
        # êµ¬ë§¤ ê°„ê²©
        df_sorted = df.sort_values('timestamp')
        time_diffs = pd.to_datetime(df_sorted['timestamp']).diff().dt.total_seconds() / 3600  # ì‹œê°„ ë‹¨ìœ„
        avg_interval = time_diffs.mean() if len(time_diffs) > 1 else 24
        min_interval = time_diffs.min() if len(time_diffs) > 1 else 24
        
        # ê°€ê²© ë¶„ì‚°
        price_variance = df['price'].var() if len(df) > 1 else 0
        price_range = df['price'].max() - df['price'].min() if len(df) > 1 else 0
        
        # ì¶©ë™ì„± ì§€í‘œ (ì§§ì€ ì‹œê°„ ë‚´ ë‹¤ìˆ˜ êµ¬ë§¤)
        impulse_score = len(df[time_diffs < 1]) / total_purchases if total_purchases > 0 else 0
        
        features = {
            'total_spent': total_spent,
            'total_purchases': total_purchases,
            'avg_amount': avg_amount,
            'time_variance': time_variance,
            'weekday_variance': weekday_variance,
            'avg_interval': avg_interval,
            'min_interval': min_interval,
            'price_variance': price_variance,
            'price_range': price_range,
            'impulse_score': impulse_score,
            'unique_items': df['name'].nunique(),
            'unique_categories': df['type'].nunique()
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¹„ìœ¨ ì¶”ê°€
        for category in ['ê°„ì‹', 'ì˜¤ë½', 'ì¥ë‚œê°', 'êµìœ¡', 'ê¸°íƒ€']:
            features[f'{category}_ratio'] = category_dist.get(category, 0) * 100
        
        return features
    
    def _calculate_change_features(self, recent: Dict, historical: Dict) -> Dict[str, float]:
        """ìµœê·¼ ë°ì´í„°ì™€ ê³¼ê±° ë°ì´í„°ì˜ ë³€í™”ëŸ‰ ê³„ì‚°"""
        changes = {}
        
        key_metrics = [
            'total_spent', 'total_purchases', 'avg_amount', 'time_variance',
            'avg_interval', 'price_variance', 'impulse_score'
        ]
        
        for metric in key_metrics:
            recent_val = recent.get(metric, 0)
            historical_val = historical.get(metric, 0)
            
            if historical_val != 0:
                change_pct = ((recent_val - historical_val) / historical_val) * 100
            else:
                change_pct = 100 if recent_val > 0 else 0
            
            changes[f'{metric}_change'] = change_pct
        
        # ì¹´í…Œê³ ë¦¬ ë³€í™”
        for category in ['ê°„ì‹', 'ì˜¤ë½', 'ì¥ë‚œê°', 'êµìœ¡', 'ê¸°íƒ€']:
            recent_ratio = recent.get(f'{category}_ratio', 0)
            historical_ratio = historical.get(f'{category}_ratio', 0)
            changes[f'{category}_shift'] = recent_ratio - historical_ratio
        
        return changes
    
    def _get_default_behavioral_features(self) -> Dict[str, float]:
        """ê¸°ë³¸ í–‰ë™ íŠ¹ì„±ê°’"""
        features = {
            'total_spent': 0, 'total_purchases': 0, 'avg_amount': 0,
            'time_variance': 0, 'weekday_variance': 0, 'avg_interval': 24,
            'min_interval': 24, 'price_variance': 0, 'price_range': 0,
            'impulse_score': 0, 'unique_items': 0, 'unique_categories': 0,
            'data_availability': 0
        }
        
        for category in ['ê°„ì‹', 'ì˜¤ë½', 'ì¥ë‚œê°', 'êµìœ¡', 'ê¸°íƒ€']:
            features[f'{category}_ratio'] = 0
            features[f'{category}_shift'] = 0
        
        for metric in ['total_spent', 'total_purchases', 'avg_amount', 'time_variance',
                      'avg_interval', 'price_variance', 'impulse_score']:
            features[f'{metric}_change'] = 0
        
        return features
    
    def _get_default_period_features(self) -> Dict[str, float]:
        """ê¸°ë³¸ ê¸°ê°„ íŠ¹ì„±ê°’"""
        features = {
            'total_spent': 0, 'total_purchases': 0, 'avg_amount': 0,
            'time_variance': 0, 'weekday_variance': 0, 'avg_interval': 24,
            'min_interval': 24, 'price_variance': 0, 'price_range': 0,
            'impulse_score': 0, 'unique_items': 0, 'unique_categories': 0
        }
        
        for category in ['ê°„ì‹', 'ì˜¤ë½', 'ì¥ë‚œê°', 'êµìœ¡', 'ê¸°íƒ€']:
            features[f'{category}_ratio'] = 0
        
        return features
    
    def train_anomaly_model(self, historical_data: List[pd.DataFrame]) -> None:
        """ê³¼ê±° ë°ì´í„°ë¡œ ì´ìƒ íƒì§€ ëª¨ë¸ í›ˆë ¨"""
        if not historical_data:
            return
        
        # ëª¨ë“  ê¸°ê°„ì˜ íŠ¹ì„± ì¶”ì¶œ
        all_features = []
        for period_data in historical_data:
            features = self.extract_behavioral_features(period_data)
            all_features.append(list(features.values()))
        
        if len(all_features) < 5:  # ìµœì†Œ ë°ì´í„° ìš”êµ¬ì‚¬í•­
            return
        
        # ì •ê·œí™”
        X = np.array(all_features)
        X_scaled = self.scaler.fit_transform(X)
        
        # ëª¨ë¸ í›ˆë ¨
        self.isolation_forest.fit(X_scaled)
        
        # ì•ˆì •ì ì¸ ë°ì´í„°ê°€ ì¶©ë¶„í•œ ê²½ìš°ì—ë§Œ EllipticEnvelope ì‚¬ìš©
        if len(all_features) >= 10:
            try:
                self.elliptic_envelope.fit(X_scaled)
            except:
                pass  # ìˆ˜ë ´í•˜ì§€ ì•ŠëŠ” ê²½ìš° ê±´ë„ˆë›°ê¸°
        
        # ëª¨ë¸ ì €ì¥
        self._save_anomaly_models()
    
    def detect_anomalies(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ì´ìƒ í–‰ë™ íƒì§€"""
        features = self.extract_behavioral_features(df)
        
        # ê¸°ë³¸ ê·œì¹™ ê¸°ë°˜ íƒì§€
        rule_based_anomalies = self._detect_rule_based_anomalies(features, df)
        
        # ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ íƒì§€ (ëª¨ë¸ì´ ìˆëŠ” ê²½ìš°)
        ml_based_anomalies = []
        try:
            self._load_anomaly_models()
            ml_based_anomalies = self._detect_ml_based_anomalies(features)
        except:
            pass  # ëª¨ë¸ì´ ì—†ê±°ë‚˜ ë¡œë“œì— ì‹¤íŒ¨í•œ ê²½ìš°
        
        # ëª¨ë“  ì´ìƒ ì§•í›„ í†µí•©
        all_anomalies = rule_based_anomalies + ml_based_anomalies
        
        # ì¤‘ë³µ ì œê±° ë° ìš°ì„ ìˆœìœ„ ì •ë ¬
        unique_anomalies = self._deduplicate_and_prioritize(all_anomalies)
        
        result = {
            'anomalies_detected': len(unique_anomalies) > 0,
            'anomaly_count': len(unique_anomalies),
            'anomalies': unique_anomalies,
            'risk_level': self._calculate_risk_level(unique_anomalies),
            'summary': self._generate_summary(unique_anomalies),
            'recommendations': self._generate_recommendations(unique_anomalies)
        }
        
        return result
    
    def _detect_rule_based_anomalies(self, features: Dict, df: pd.DataFrame) -> List[Dict]:
        """ê·œì¹™ ê¸°ë°˜ ì´ìƒ íƒì§€"""
        anomalies = []
        
        # 1. ê¸‰ê²©í•œ ì§€ì¶œ ì¦ê°€
        spending_change = features.get('total_spent_change', 0)
        if spending_change > 200:  # 200% ì´ìƒ ì¦ê°€
            anomalies.append({
                'type': 'spending_spike',
                'severity': 'warning',
                'confidence': min(spending_change / 300, 1.0),
                'details': f'ì§€ì¶œì´ {spending_change:.1f}% ì¦ê°€í–ˆìŠµë‹ˆë‹¤',
                'timestamp': datetime.now()
            })
        
        # 2. êµ¬ë§¤ ë¹ˆë„ ê¸‰ë³€
        frequency_change = features.get('total_purchases_change', 0)
        if abs(frequency_change) > 150:  # 150% ì´ìƒ ë³€í™”
            anomalies.append({
                'type': 'frequency_change',
                'severity': 'warning',
                'confidence': min(abs(frequency_change) / 200, 1.0),
                'details': f'êµ¬ë§¤ ë¹ˆë„ê°€ {frequency_change:.1f}% ë³€í™”í–ˆìŠµë‹ˆë‹¤',
                'timestamp': datetime.now()
            })
        
        # 3. ì¹´í…Œê³ ë¦¬ ê¸‰ê²©í•œ ë³€í™”
        for category in ['ê°„ì‹', 'ì˜¤ë½', 'ì¥ë‚œê°', 'êµìœ¡', 'ê¸°íƒ€']:
            shift = features.get(f'{category}_shift', 0)
            if abs(shift) > 30:  # 30% ì´ìƒ ë¹„ì¤‘ ë³€í™”
                anomalies.append({
                    'type': 'category_shift',
                    'severity': 'info',
                    'confidence': min(abs(shift) / 50, 1.0),
                    'details': f'{category} ì¹´í…Œê³ ë¦¬ ë¹„ì¤‘ì´ {shift:+.1f}% ë³€í™”í–ˆìŠµë‹ˆë‹¤',
                    'timestamp': datetime.now()
                })
        
        # 4. ì¶©ë™ êµ¬ë§¤ íŒ¨í„´
        impulse_score = features.get('impulse_score', 0)
        if impulse_score > 0.3:  # 30% ì´ìƒì´ 1ì‹œê°„ ë‚´ êµ¬ë§¤
            anomalies.append({
                'type': 'impulse_buying',
                'severity': 'warning',
                'confidence': min(impulse_score / 0.5, 1.0),
                'details': f'êµ¬ë§¤ì˜ {impulse_score*100:.1f}%ê°€ ì§§ì€ ì‹œê°„ ë‚´ì— ë°œìƒí–ˆìŠµë‹ˆë‹¤',
                'timestamp': datetime.now()
            })
        
        # 5. ì‹œê°„ íŒ¨í„´ ë³€í™”
        time_variance_change = features.get('time_variance_change', 0)
        if abs(time_variance_change) > 100:
            anomalies.append({
                'type': 'time_pattern_shift',
                'severity': 'info',
                'confidence': min(abs(time_variance_change) / 150, 1.0),
                'details': 'êµ¬ë§¤ ì‹œê°„ íŒ¨í„´ì´ í‰ì†Œì™€ ë‹¤ë¦…ë‹ˆë‹¤',
                'timestamp': datetime.now()
            })
        
        # 6. ê°ì •ì  ì†Œë¹„ íŒ¨í„´ (ë³µí•© ì§€í‘œ)
        emotional_indicators = [
            impulse_score > 0.25,
            spending_change > 150,
            features.get('ê°„ì‹_shift', 0) > 20,  # ê°„ì‹ ê¸‰ì¦
            features.get('min_interval', 24) < 2  # ë§¤ìš° ì§§ì€ êµ¬ë§¤ ê°„ê²©
        ]
        
        if sum(emotional_indicators) >= 2:
            anomalies.append({
                'type': 'emotional_shopping',
                'severity': 'alert',
                'confidence': sum(emotional_indicators) / 4,
                'details': 'ê°ì •ì  ë³€í™”ë¡œ ì¸í•œ ì†Œë¹„ íŒ¨í„´ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤',
                'timestamp': datetime.now()
            })
        
        return anomalies
    
    def _detect_ml_based_anomalies(self, features: Dict) -> List[Dict]:
        """ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì´ìƒ íƒì§€"""
        anomalies = []
        
        feature_vector = np.array([list(features.values())]).reshape(1, -1)
        feature_scaled = self.scaler.transform(feature_vector)
        
        # Isolation Forest ì˜ˆì¸¡
        isolation_prediction = self.isolation_forest.predict(feature_scaled)[0]
        isolation_score = self.isolation_forest.decision_function(feature_scaled)[0]
        
        if isolation_prediction == -1:  # ì´ìƒì¹˜ë¡œ ë¶„ë¥˜
            confidence = min(abs(isolation_score) / 0.3, 1.0)
            anomalies.append({
                'type': 'ml_detected_anomaly',
                'severity': 'warning',
                'confidence': confidence,
                'details': f'AIê°€ ì´ìƒ íŒ¨í„´ì„ ê°ì§€í–ˆìŠµë‹ˆë‹¤ (ì ìˆ˜: {isolation_score:.3f})',
                'timestamp': datetime.now()
            })
        
        return anomalies
    
    def _deduplicate_and_prioritize(self, anomalies: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ì œê±° ë° ìš°ì„ ìˆœìœ„ ì •ë ¬"""
        if not anomalies:
            return []
        
        # ì‹¬ê°ë„ë³„ ìš°ì„ ìˆœìœ„
        severity_order = {'alert': 3, 'warning': 2, 'info': 1}
        
        # ì •ë ¬: ì‹¬ê°ë„ > ì‹ ë¢°ë„ ìˆœ
        sorted_anomalies = sorted(
            anomalies,
            key=lambda x: (severity_order.get(x['severity'], 0), x['confidence']),
            reverse=True
        )
        
        # ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ë°˜í™˜
        return sorted_anomalies[:5]
    
    def _calculate_risk_level(self, anomalies: List[Dict]) -> str:
        """ì „ì²´ ìœ„í—˜ ìˆ˜ì¤€ ê³„ì‚°"""
        if not anomalies:
            return 'low'
        
        alert_count = sum(1 for a in anomalies if a['severity'] == 'alert')
        warning_count = sum(1 for a in anomalies if a['severity'] == 'warning')
        
        if alert_count > 0:
            return 'high'
        elif warning_count >= 2:
            return 'medium'
        elif warning_count > 0:
            return 'medium'
        else:
            return 'low'
    
    def _generate_summary(self, anomalies: List[Dict]) -> str:
        """ì´ìƒ ì§•í›„ ìš”ì•½"""
        if not anomalies:
            return "ì •ìƒì ì¸ êµ¬ë§¤ íŒ¨í„´ì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤."
        
        alert_count = sum(1 for a in anomalies if a['severity'] == 'alert')
        warning_count = sum(1 for a in anomalies if a['severity'] == 'warning')
        
        if alert_count > 0:
            return f"ì£¼ì˜ê°€ í•„ìš”í•œ {alert_count}ê°œì˜ íŒ¨í„´ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
        elif warning_count > 0:
            return f"{warning_count}ê°œì˜ ë³€í™” íŒ¨í„´ì´ ê´€ì°°ë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            return "ì¼ë¶€ ë³€í™”ê°€ ìˆì§€ë§Œ ì •ìƒ ë²”ìœ„ ë‚´ì…ë‹ˆë‹¤."
    
    def _generate_recommendations(self, anomalies: List[Dict]) -> List[str]:
        """ì´ìƒ ì§•í›„ë³„ ê¶Œì¥ì‚¬í•­"""
        recommendations = set()
        
        for anomaly in anomalies:
            anomaly_type = anomaly['type']
            
            if anomaly_type == 'spending_spike':
                recommendations.add("ì˜ˆì‚° í•œë„ë¥¼ ë‹¤ì‹œ ê²€í† í•´ë³´ì„¸ìš”")
                recommendations.add("ì•„ì´ì™€ ì†Œë¹„ ê³„íšì„ í•¨ê»˜ ì„¸ì›Œë³´ì„¸ìš”")
            
            elif anomaly_type == 'impulse_buying':
                recommendations.add("êµ¬ë§¤ ì „ ì ì‹œ ìƒê°í•  ì‹œê°„ì„ ê°€ì ¸ë³´ì„¸ìš”")
                recommendations.add("êµ¬ë§¤ ëª©ë¡ì„ ë¯¸ë¦¬ ì‘ì„±í•˜ëŠ” ìŠµê´€ì„ ë§Œë“¤ì–´ë³´ì„¸ìš”")
            
            elif anomaly_type == 'emotional_shopping':
                recommendations.add("ì•„ì´ì˜ ê°ì • ìƒíƒœë¥¼ ì‚´í´ë³´ì„¸ìš”")
                recommendations.add("ìŠ¤íŠ¸ë ˆìŠ¤ë‚˜ ë³€í™”ê°€ ìˆì—ˆëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”")
                recommendations.add("ê°ì •ì„ í‘œí˜„í•  ë‹¤ë¥¸ ë°©ë²•ì„ ì°¾ì•„ë³´ì„¸ìš”")
            
            elif anomaly_type == 'category_shift':
                recommendations.add("ìƒˆë¡œìš´ ê´€ì‹¬ì‚¬ê°€ ìƒê²¼ëŠ”ì§€ ëŒ€í™”í•´ë³´ì„¸ìš”")
                recommendations.add("ì¹´í…Œê³ ë¦¬ ê· í˜•ì„ ë§ì¶°ë³´ì„¸ìš”")
            
            elif anomaly_type == 'frequency_change':
                recommendations.add("êµ¬ë§¤ íŒ¨í„´ì˜ ë³€í™” ì›ì¸ì„ íŒŒì•…í•´ë³´ì„¸ìš”")
                recommendations.add("ê·œì¹™ì ì¸ êµ¬ë§¤ ìŠ¤ì¼€ì¤„ì„ ë§Œë“¤ì–´ë³´ì„¸ìš”")
        
        # ì¼ë°˜ì ì¸ ê¶Œì¥ì‚¬í•­ ì¶”ê°€
        if anomalies:
            recommendations.add("ì •ê¸°ì ìœ¼ë¡œ êµ¬ë§¤ íŒ¨í„´ì„ ë¦¬ë·°í•´ë³´ì„¸ìš”")
        
        return list(recommendations)
    
    def get_anomaly_alerts(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """ì•Œë¦¼ìš© ì´ìƒ ì§•í›„ ì •ë³´"""
        detection_result = self.detect_anomalies(df)
        alerts = []
        
        if not detection_result['anomalies_detected']:
            return alerts
        
        for anomaly in detection_result['anomalies']:
            anomaly_info = self.anomaly_types.get(anomaly['type'], {
                'title': 'íŒ¨í„´ ë³€í™” ê°ì§€',
                'description': 'êµ¬ë§¤ íŒ¨í„´ì— ë³€í™”ê°€ ìˆìŠµë‹ˆë‹¤',
                'severity': anomaly['severity'],
                'icon': 'ğŸ”'
            })
            
            alert = {
                'type': anomaly['severity'],
                'title': f"{anomaly_info['icon']} {anomaly_info['title']}",
                'message': anomaly.get('details', anomaly_info['description']),
                'confidence': anomaly['confidence'],
                'timestamp': anomaly['timestamp'],
                'recommendations': self._get_specific_recommendations(anomaly['type'])
            }
            
            alerts.append(alert)
        
        return alerts
    
    def _get_specific_recommendations(self, anomaly_type: str) -> List[str]:
        """íŠ¹ì • ì´ìƒ ìœ í˜•ì— ëŒ€í•œ êµ¬ì²´ì  ê¶Œì¥ì‚¬í•­"""
        recommendations_map = {
            'spending_spike': [
                "ì´ë²ˆ ì£¼ ì˜ˆì‚°ì„ ì¬ê²€í† í•´ë³´ì„¸ìš”",
                "í° êµ¬ë§¤ ì „ì—ëŠ” í•˜ë£¨ ì •ë„ ìƒê°í•´ë³´ì„¸ìš”"
            ],
            'impulse_buying': [
                "êµ¬ë§¤ ì „ ëª©ë¡ ì‘ì„± ìŠµê´€ì„ ë§Œë“¤ì–´ë³´ì„¸ìš”",
                "ì•„ì´ì™€ í•¨ê»˜ êµ¬ë§¤ ì´ìœ ë¥¼ ì´ì•¼ê¸°í•´ë³´ì„¸ìš”"
            ],
            'emotional_shopping': [
                "ì•„ì´ì˜ ê°ì • ìƒíƒœë¥¼ ì²´í¬í•´ë³´ì„¸ìš”",
                "ëŒ€í™”ë¥¼ í†µí•´ ìŠ¤íŠ¸ë ˆìŠ¤ ì›ì¸ì„ ì°¾ì•„ë³´ì„¸ìš”"
            ],
            'category_shift': [
                "ìƒˆë¡œìš´ ê´€ì‹¬ì‚¬ì— ëŒ€í•´ ëŒ€í™”í•´ë³´ì„¸ìš”",
                "ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ì˜ ê· í˜•ì„ ë§ì¶°ë³´ì„¸ìš”"
            ]
        }
        
        return recommendations_map.get(anomaly_type, [
            "íŒ¨í„´ ë³€í™”ì˜ ì›ì¸ì„ íŒŒì•…í•´ë³´ì„¸ìš”",
            "ì•„ì´ì™€ í•¨ê»˜ ì†Œë¹„ ìŠµê´€ì„ ì ê²€í•´ë³´ì„¸ìš”"
        ])
    
    def _save_anomaly_models(self):
        """ì´ìƒ íƒì§€ ëª¨ë¸ ì €ì¥"""
        joblib.dump(self.isolation_forest, self.model_path / "isolation_forest.pkl")
        joblib.dump(self.scaler, self.model_path / "anomaly_scaler.pkl")
        try:
            joblib.dump(self.elliptic_envelope, self.model_path / "elliptic_envelope.pkl")
        except:
            pass
    
    def _load_anomaly_models(self):
        """ì €ì¥ëœ ì´ìƒ íƒì§€ ëª¨ë¸ ë¡œë“œ"""
        self.isolation_forest = joblib.load(self.model_path / "isolation_forest.pkl")
        self.scaler = joblib.load(self.model_path / "anomaly_scaler.pkl")
        try:
            self.elliptic_envelope = joblib.load(self.model_path / "elliptic_envelope.pkl")
        except:
            pass
